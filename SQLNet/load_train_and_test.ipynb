{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting babel==2.9.1\n",
      "  Downloading Babel-2.9.1-py2.py3-none-any.whl (8.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.8 MB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: defusedxml==0.7.1 in /home/apoorv/.local/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (0.7.1)\n",
      "Collecting docopt==0.6.2\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Requirement already satisfied: et-xmlfile==1.1.0 in /home/apoorv/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (1.1.0)\n",
      "Collecting greenlet==1.1.2\n",
      "  Downloading greenlet-1.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
      "\u001b[K     |████████████████████████████████| 156 kB 18.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jdcal==1.4.1\n",
      "  Downloading jdcal-1.4.1-py2.py3-none-any.whl (9.5 kB)\n",
      "Collecting numpy==1.22.2\n",
      "  Using cached numpy-1.22.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "Collecting odfpy==1.4.1\n",
      "  Downloading odfpy-1.4.1.tar.gz (717 kB)\n",
      "\u001b[K     |████████████████████████████████| 717 kB 13.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting olefile==0.46\n",
      "  Using cached olefile-0.46.zip (112 kB)\n",
      "Collecting openpyxl==2.4.11\n",
      "  Downloading openpyxl-2.4.11.tar.gz (158 kB)\n",
      "\u001b[K     |████████████████████████████████| 158 kB 14.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz==2021.3 in /home/apoorv/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from -r requirements.txt (line 98)) (2021.3)\n",
      "Collecting records==0.5.3\n",
      "  Downloading records-0.5.3-py2.py3-none-any.whl (10 kB)\n",
      "Collecting sqlalchemy==1.4.31\n",
      "  Downloading SQLAlchemy-1.4.31-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 19.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tablib==3.2.0\n",
      "  Downloading tablib-3.2.0-py3-none-any.whl (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 2.4 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting torch==1.10.2\n",
      "  Downloading torch-1.10.2-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 881.9 MB 1.8 kB/s eta 0:00:017     |████████████████▏               | 445.4 MB 2.4 MB/s eta 0:03:05     |███████████████████████▋        | 652.0 MB 9.1 MB/s eta 0:00:26\n",
      "\u001b[?25hCollecting typing-extensions==4.1.1\n",
      "  Using cached typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
      "Collecting unicodecsv==0.14.1\n",
      "  Downloading unicodecsv-0.14.1.tar.gz (10 kB)\n",
      "Requirement already satisfied: xlrd==2.0.1 in /home/apoorv/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from -r requirements.txt (line 169)) (2.0.1)\n",
      "Collecting xlwt==1.3.0\n",
      "  Downloading xlwt-1.3.0-py2.py3-none-any.whl (99 kB)\n",
      "\u001b[K     |████████████████████████████████| 99 kB 2.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hUsing legacy setup.py install for docopt, since package 'wheel' is not installed.\n",
      "Using legacy setup.py install for odfpy, since package 'wheel' is not installed.\n",
      "Using legacy setup.py install for olefile, since package 'wheel' is not installed.\n",
      "Using legacy setup.py install for openpyxl, since package 'wheel' is not installed.\n",
      "Using legacy setup.py install for unicodecsv, since package 'wheel' is not installed.\n",
      "\u001b[31mERROR: thinc 7.4.5 requires tqdm<5.0.0,>=4.10.0, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: thinc 7.4.5 requires wasabi<1.1.0,>=0.0.9, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: tensorboardx 2.1 requires protobuf>=3.8.0, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: streamlit 0.84.0 requires blinker, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: streamlit 0.84.0 requires pillow>=6.2.0, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: streamlit 0.84.0 requires protobuf!=3.11,>=3.6.0, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: spacy 2.3.5 requires tqdm<5.0.0,>=4.38.0, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: spacy 2.3.5 requires wasabi<1.1.0,>=0.4.0, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: altair 4.1.0 requires entrypoints, which is not installed.\u001b[0m\n",
      "Installing collected packages: babel, docopt, greenlet, jdcal, numpy, odfpy, olefile, openpyxl, sqlalchemy, tablib, records, typing-extensions, torch, unicodecsv, xlwt\n",
      "    Running setup.py install for docopt ... \u001b[?25ldone\n",
      "\u001b[?25h  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.0\n",
      "    Uninstalling numpy-1.21.0:\n",
      "      Successfully uninstalled numpy-1.21.0\n",
      "    Running setup.py install for odfpy ... \u001b[?25ldone\n",
      "\u001b[?25h    Running setup.py install for olefile ... \u001b[?25ldone\n",
      "\u001b[?25h  Attempting uninstall: openpyxl\n",
      "    Found existing installation: openpyxl 3.0.9\n",
      "    Uninstalling openpyxl-3.0.9:\n",
      "      Successfully uninstalled openpyxl-3.0.9\n",
      "    Running setup.py install for openpyxl ... \u001b[?25ldone\n",
      "\u001b[?25h  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.7.4.3\n",
      "    Uninstalling typing-extensions-3.7.4.3:\n",
      "      Successfully uninstalled typing-extensions-3.7.4.3\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.7.1\n",
      "    Uninstalling torch-1.7.1:\n",
      "      Successfully uninstalled torch-1.7.1\n",
      "    Running setup.py install for unicodecsv ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed babel-2.9.1 docopt-0.6.2 greenlet-1.1.2 jdcal-1.4.1 numpy-1.22.2 odfpy-1.4.1 olefile-0.46 openpyxl-2.4.11 records-0.5.3 sqlalchemy-1.4.31 tablib-3.2.0 torch-1.10.2 typing-extensions-4.1.1 unicodecsv-0.14.1 xlwt-1.3.0\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/home/apoorv/.pyenv/versions/3.8.5/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# To install dependendcies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/\n",
      "data/test_tok.jsonl\n",
      "data/test_tok.tables.jsonl\n",
      "data/dev.db\n",
      "data/train_tok.jsonl\n",
      "data/test.db\n",
      "data/train_tok.tables.jsonl\n",
      "data/dev_tok.jsonl\n",
      "data/train.db\n",
      "data/dev_tok.tables.jsonl\n",
      "data_resplit/\n",
      "data_resplit/table.db\n",
      "data_resplit/test.jsonl\n",
      "data_resplit/tables.jsonl\n",
      "data_resplit/train.jsonl\n",
      "data_resplit/dev.jsonl\n"
     ]
    }
   ],
   "source": [
    "#to extract data\n",
    "!tar -xjvf ./data/data.tar.bz2 -C ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-22 09:50:40--  http://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip [following]\n",
      "--2022-03-22 09:50:41--  https://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.42B.300d.zip [following]\n",
      "--2022-03-22 09:50:43--  http://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.42B.300d.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1877802108 (1.7G) [application/zip]\n",
      "Saving to: ‘./glove/glove.42B.300d.zip’\n",
      "\n",
      "glove.42B.300d.zip   34%[=====>              ] 611.02M   260KB/s    eta 74m 57s"
     ]
    }
   ],
   "source": [
    "!wget -P ./glove http://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./glove/glove.42B.300d.zip\n",
      "  inflating: ./glove/glove.42B.300d.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip ./glove/glove.42B.300d.zip -d ./glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/apoorv/.local/lib/python3.8/site-packages/pandas/compat/__init__.py:109: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "Loading from original dataset\n",
      "Loading data from data/data/train_tok.jsonl\n",
      "Loading data from data/data/train_tok.tables.jsonl\n",
      "Loading data from data/data/dev_tok.jsonl\n",
      "Loading data from data/data/dev_tok.tables.jsonl\n",
      "Loading data from data/data/test_tok.jsonl\n",
      "Loading data from data/data/test_tok.tables.jsonl\n",
      "Loading word embedding from glove/glove.42B.300d.txt\n",
      "Killed\n"
     ]
    }
   ],
   "source": [
    "!python extract_vocab.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: records in /home/apoorv/.pyenv/versions/3.8.5/lib/python3.8/site-packages (0.5.3)\n",
      "Requirement already satisfied: docopt in /home/apoorv/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from records) (0.6.2)\n",
      "Requirement already satisfied: SQLAlchemy; python_version >= \"3.0\" in /home/apoorv/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from records) (1.4.31)\n",
      "Requirement already satisfied: tablib>=0.11.4 in /home/apoorv/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from records) (3.2.0)\n",
      "Requirement already satisfied: openpyxl<2.5.0 in /home/apoorv/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from records) (2.4.11)\n",
      "Requirement already satisfied: greenlet!=0.4.17; python_version >= \"3\" and (platform_machine == \"aarch64\" or (platform_machine == \"ppc64le\" or (platform_machine == \"x86_64\" or (platform_machine == \"amd64\" or (platform_machine == \"AMD64\" or (platform_machine == \"win32\" or platform_machine == \"WIN32\")))))) in /home/apoorv/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from SQLAlchemy; python_version >= \"3.0\"->records) (1.1.2)\n",
      "Requirement already satisfied: jdcal in /home/apoorv/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from openpyxl<2.5.0->records) (1.4.1)\n",
      "Requirement already satisfied: et_xmlfile in /home/apoorv/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from openpyxl<2.5.0->records) (1.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/home/apoorv/.pyenv/versions/3.8.5/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from sqlnet.utils import *\n",
    "from sqlnet.model.seq2sql import Seq2SQL\n",
    "from sqlnet.model.sqlnet import SQLNet\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_word=300\n",
    "B_word=42\n",
    "# False in case in which you want to use full dataset\n",
    "USE_SMALL=True\n",
    "GPU=True\n",
    "BATCH_SIZE=16\n",
    "TRAIN_ENTRY=(True, True, True)  # (AGG, SEL, COND)\n",
    "TRAIN_AGG, TRAIN_SEL, TRAIN_COND = TRAIN_ENTRY\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sql_paths, table_paths, use_small=False):\n",
    "    if not isinstance(sql_paths, list):\n",
    "        sql_paths = (sql_paths, )\n",
    "    if not isinstance(table_paths, list):\n",
    "        table_paths = (table_paths, )\n",
    "    sql_data = []\n",
    "    table_data = {}\n",
    "\n",
    "    max_col_num = 0\n",
    "    for SQL_PATH in sql_paths:\n",
    "        print(\"Loading data from %s\"%SQL_PATH)\n",
    "        with open(SQL_PATH) as inf:\n",
    "            for idx, line in enumerate(inf):\n",
    "                if use_small and idx >= 1000:\n",
    "                    break\n",
    "                sql = json.loads(line.strip())\n",
    "                sql_data.append(sql)\n",
    "\n",
    "    for TABLE_PATH in table_paths:\n",
    "        print(\"Loading data from %s\"%TABLE_PATH)\n",
    "        with open(TABLE_PATH) as inf:\n",
    "            for line in inf:\n",
    "                tab = json.loads(line.strip())\n",
    "                table_data[tab[u'id']] = tab\n",
    "\n",
    "    for sql in sql_data:\n",
    "        assert sql[u'table_id'] in table_data\n",
    "\n",
    "    return sql_data, table_data\n",
    "def load_dataset(dataset_id, use_small=False):\n",
    "    if dataset_id == 0:\n",
    "        print(\"Loading from original dataset\")\n",
    "        sql_data, table_data = load_data('data/data/train_tok.jsonl',\n",
    "                'data/data/train_tok.tables.jsonl', use_small=use_small)\n",
    "        val_sql_data, val_table_data = load_data('data/data/dev_tok.jsonl',\n",
    "                'data/data/dev_tok.tables.jsonl', use_small=use_small)\n",
    "        test_sql_data, test_table_data = load_data('data/data/test_tok.jsonl',\n",
    "                'data/data/test_tok.tables.jsonl', use_small=use_small)\n",
    "        TRAIN_DB = 'data/data/train.db'\n",
    "        DEV_DB = 'data/data/dev.db'\n",
    "        TEST_DB = 'data/data/test.db'\n",
    "    else:\n",
    "        print(\"Loading from re-split dataset\")\n",
    "        sql_data, table_data = load_data('data/data_resplit/train.jsonl',\n",
    "                'data/data_resplit/tables.jsonl', use_small=use_small)\n",
    "        val_sql_data, val_table_data = load_data('data/data_resplit/dev.jsonl',\n",
    "                'data/data_resplit/tables.jsonl', use_small=use_small)\n",
    "        test_sql_data, test_table_data = load_data('data/data_resplit/test.jsonl',\n",
    "                'data/data_resplit/tables.jsonl', use_small=use_small)\n",
    "        TRAIN_DB = 'data/data_resplit/table.db'\n",
    "        DEV_DB = 'data/data_resplit/table.db'\n",
    "        TEST_DB = 'data/data_resplit/table.db'\n",
    "\n",
    "    return sql_data, table_data, val_sql_data, val_table_data,\\\n",
    "            test_sql_data, test_table_data, TRAIN_DB, DEV_DB, TEST_DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_emb(file_name, load_used=False, use_small=False):\n",
    "    if not load_used:\n",
    "        print ('Loading word embedding from %s'%file_name)\n",
    "        ret = {}\n",
    "        with open(file_name) as inf:\n",
    "            for idx, line in enumerate(inf):\n",
    "                if (use_small and idx >= 5000):\n",
    "                    break\n",
    "                info = line.strip().split(' ')\n",
    "                if info[0].lower() not in ret:\n",
    "                    ret[info[0]] = np.array(map(lambda x:float(x), info[1:]))\n",
    "        return ret\n",
    "    else:\n",
    "        print ('Load used word embedding')\n",
    "        with open('glove/word2idx.json') as inf:\n",
    "            w2i = json.load(inf)\n",
    "        with open('glove/usedwordemb.npy') as inf:\n",
    "            word_emb_val = np.load(inf)\n",
    "        return w2i, word_emb_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from original dataset\n",
      "Loading data from data/data/train_tok.jsonl\n",
      "Loading data from data/data/train_tok.tables.jsonl\n",
      "Loading data from data/data/dev_tok.jsonl\n",
      "Loading data from data/data/dev_tok.tables.jsonl\n",
      "Loading data from data/data/test_tok.jsonl\n",
      "Loading data from data/data/test_tok.tables.jsonl\n",
      "Load used word embedding\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove/word2idx.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-764fd25562e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m word_emb = load_word_emb('./glove/glove.%dB.%dd.txt'%(B_word,N_word), \\\n\u001b[0;32m----> 7\u001b[0;31m         True, use_small=USE_SMALL)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-3b237e45f6d6>\u001b[0m in \u001b[0;36mload_word_emb\u001b[0;34m(file_name, load_used, use_small)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Load used word embedding'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove/word2idx.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mw2i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove/usedwordemb.npy'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove/word2idx.json'"
     ]
    }
   ],
   "source": [
    "sql_data, table_data, val_sql_data, val_table_data, \\\n",
    "        test_sql_data, test_table_data, \\\n",
    "        TRAIN_DB, DEV_DB, TEST_DB = load_dataset(\n",
    "                0, use_small=USE_SMALL)\n",
    "\n",
    "word_emb = load_word_emb('./glove/glove.%dB.%dd.txt'%(B_word,N_word), \\\n",
    "        True, use_small=USE_SMALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model_name(for_load=False):\n",
    "    new_data = 'old'\n",
    "    mode = 'sqlnet'\n",
    "    train_emb = False\n",
    "    if for_load:\n",
    "        use_emb = use_rl = ''\n",
    "    else:\n",
    "        use_emb = ''\n",
    "        use_rl = ''\n",
    "    use_ca = '_ca'\n",
    "\n",
    "    agg_model_name = 'saved_model/%s_%s%s%s.agg_model'%(new_data,\n",
    "            mode, use_emb, use_ca)\n",
    "    sel_model_name = 'saved_model/%s_%s%s%s.sel_model'%(new_data,\n",
    "            mode, use_emb, use_ca)\n",
    "    cond_model_name = 'saved_model/%s_%s%s%s.cond_%smodel'%(new_data,\n",
    "            mode, use_emb, use_ca, use_rl)\n",
    "\n",
    "    if not for_load and train_emb:\n",
    "        agg_embed_name = 'saved_model/%s_%s%s%s.agg_embed'%(new_data,\n",
    "                mode, use_emb, use_ca)\n",
    "        sel_embed_name = 'saved_model/%s_%s%s%s.sel_embed'%(new_data,\n",
    "                mode, use_emb, use_ca)\n",
    "        cond_embed_name = 'saved_model/%s_%s%s%s.cond_embed'%(new_data,\n",
    "                mode, use_emb, use_ca)\n",
    "\n",
    "        return agg_model_name, sel_model_name, cond_model_name,\\\n",
    "                agg_embed_name, sel_embed_name, cond_embed_name\n",
    "    else:\n",
    "        return agg_model_name, sel_model_name, cond_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SQLNet(word_emb, N_word=N_word, use_ca=True,\n",
    "                gpu=GPU, trainable_emb = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "            lr=learning_rate, weight_decay = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_m, sel_m, cond_m = best_model_name()\n",
    "train_emb = False\n",
    "suffix = \"trained\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f25966370baa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m init_acc = epoch_acc(model, BATCH_SIZE,\n\u001b[0m\u001b[1;32m      2\u001b[0m                 val_sql_data, val_table_data, TRAIN_ENTRY)\n\u001b[1;32m      3\u001b[0m \u001b[0mbest_agg_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbest_agg_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbest_sel_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "init_acc = epoch_acc(model, BATCH_SIZE,\n",
    "                val_sql_data, val_table_data, TRAIN_ENTRY)\n",
    "best_agg_acc = init_acc[1][0]\n",
    "best_agg_idx = 0\n",
    "best_sel_acc = init_acc[1][1]\n",
    "best_sel_idx = 0\n",
    "best_cond_acc = init_acc[1][2]\n",
    "best_cond_idx = 0\n",
    "print('Init dev acc_qm: %s\\n  breakdown on (agg, sel, where): %s'%\\\n",
    "        init_acc)\n",
    "if TRAIN_AGG:\n",
    "    torch.save(model.agg_pred.state_dict(), agg_m)\n",
    "    if train_emb:\n",
    "        torch.save(model.agg_embed_layer.state_dict(), agg_e)\n",
    "if TRAIN_SEL:\n",
    "    torch.save(model.sel_pred.state_dict(), sel_m)\n",
    "    if train_emb:\n",
    "        torch.save(model.sel_embed_layer.state_dict(), sel_e)\n",
    "if TRAIN_COND:\n",
    "    torch.save(model.cond_pred.state_dict(), cond_m)\n",
    "    if train_emb:\n",
    "        torch.save(model.cond_embed_layer.state_dict(), cond_e)\n",
    "for i in range(100):\n",
    "    print('Epoch %d @ %s'%(i+1, datetime.datetime.now()))\n",
    "    print(' Loss = %s'%epoch_train(\n",
    "            model, optimizer, BATCH_SIZE, \n",
    "            sql_data, table_data, TRAIN_ENTRY))\n",
    "    print(' Train acc_qm: %s\\n   breakdown result: %s'%epoch_acc(\n",
    "            model, BATCH_SIZE, sql_data, table_data, TRAIN_ENTRY))\n",
    "    #val_acc = epoch_token_acc(model, BATCH_SIZE, val_sql_data, val_table_data, TRAIN_ENTRY)\n",
    "    val_acc = epoch_acc(model,\n",
    "            BATCH_SIZE, val_sql_data, val_table_data, TRAIN_ENTRY)\n",
    "    print (' Dev acc_qm: %s\\n   breakdown result: %s'%val_acc)\n",
    "    if TRAIN_AGG:\n",
    "        if val_acc[1][0] > best_agg_acc:\n",
    "            best_agg_acc = val_acc[1][0]\n",
    "            best_agg_idx = i+1\n",
    "            torch.save(model.agg_pred.state_dict(),\n",
    "                'saved_model/epoch%d.agg_model%s'%(i+1, suffix))\n",
    "            torch.save(model.agg_pred.state_dict(), agg_m)\n",
    "            if train_emb:\n",
    "                torch.save(model.agg_embed_layer.state_dict(),\n",
    "                'saved_model/epoch%d.agg_embed%s'%(i+1,suffix))\n",
    "                torch.save(model.agg_embed_layer.state_dict(), agg_e)\n",
    "    if TRAIN_SEL:\n",
    "        if val_acc[1][1] > best_sel_acc:\n",
    "            best_sel_acc = val_acc[1][1]\n",
    "            best_sel_idx = i+1\n",
    "            torch.save(model.sel_pred.state_dict(),\n",
    "                'saved_model/epoch%d.sel_model%s'%(i+1, suffix))\n",
    "            torch.save(model.sel_pred.state_dict(), sel_m)\n",
    "            if train_emb:\n",
    "                torch.save(model.sel_embed_layer.state_dict(),\n",
    "                'saved_model/epoch%d.sel_embed%s'%(i+1, suffix))\n",
    "                torch.save(model.sel_embed_layer.state_dict(), sel_e)\n",
    "    if TRAIN_COND:\n",
    "        if val_acc[1][2] > best_cond_acc:\n",
    "            best_cond_acc = val_acc[1][2]\n",
    "            best_cond_idx = i+1\n",
    "            torch.save(model.cond_pred.state_dict(),\n",
    "                'saved_model/epoch%d.cond_model%s'%(i+1,suffix))\n",
    "            torch.save(model.cond_pred.state_dict(), cond_m)\n",
    "            if train_emb:\n",
    "                torch.save(model.cond_embed_layer.state_dict(),\n",
    "                'saved_model/epoch%d.cond_embed%s'%(i+1, suffix))\n",
    "                torch.save(model.cond_embed_layer.state_dict(), cond_e)\n",
    "    print(' Best val acc = %s, on epoch %s individually'%(\n",
    "            (best_agg_acc, best_sel_acc, best_cond_acc),\n",
    "            (best_agg_idx, best_sel_idx, best_cond_idx)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from original dataset\n",
      "Loading data from data/data/train_tok.jsonl\n",
      "Loading data from data/data/train_tok.tables.jsonl\n",
      "Loading data from data/data/dev_tok.jsonl\n",
      "Loading data from data/data/dev_tok.tables.jsonl\n",
      "Loading data from data/data/test_tok.jsonl\n",
      "Loading data from data/data/test_tok.tables.jsonl\n",
      "Load used word embedding\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove/word2idx.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-240f7512876b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m word_emb = load_word_emb('glove/glove.%dB.%dd.txt'%(B_word,N_word), \\\n\u001b[0;32m---> 10\u001b[0;31m     load_used=True, use_small=USE_SMALL) # load_used can speed up loading\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-3b237e45f6d6>\u001b[0m in \u001b[0;36mload_word_emb\u001b[0;34m(file_name, load_used, use_small)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Load used word embedding'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove/word2idx.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mw2i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove/usedwordemb.npy'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove/word2idx.json'"
     ]
    }
   ],
   "source": [
    "#for testing\n",
    "TEST_ENTRY=(True, True, True)  # (AGG, SEL, COND)\n",
    "\n",
    "sql_data, table_data, val_sql_data, val_table_data, \\\n",
    "        test_sql_data, test_table_data, \\\n",
    "        TRAIN_DB, DEV_DB, TEST_DB = load_dataset(\n",
    "                0, use_small=USE_SMALL)\n",
    "\n",
    "word_emb = load_word_emb('glove/glove.%dB.%dd.txt'%(B_word,N_word), \\\n",
    "    load_used=True, use_small=USE_SMALL) # load_used can speed up loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SQLNet(word_emb, N_word=N_word, use_ca=True, gpu=GPU,\n",
    "                trainable_emb = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_m, sel_m, cond_m = best_model_name()\n",
    "print(\"Loading from %s\"%agg_m)\n",
    "model.agg_pred.load_state_dict(torch.load(agg_m))\n",
    "print(\"Loading from %s\"%sel_m)\n",
    "model.sel_pred.load_state_dict(torch.load(sel_m))\n",
    "print(\"Loading from %s\"%cond_m)\n",
    "model.cond_pred.load_state_dict(torch.load(cond_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dev acc_qm: %s;\\n  breakdown on (agg, sel, where): %s\"%epoch_acc(\n",
    "            model, BATCH_SIZE, val_sql_data, val_table_data, TEST_ENTRY))\n",
    "print(\"Dev execution acc: %s\"%epoch_exec_acc(\n",
    "        model, BATCH_SIZE, val_sql_data, val_table_data, DEV_DB))\n",
    "print(\"Test acc_qm: %s;\\n  breakdown on (agg, sel, where): %s\"%epoch_acc(\n",
    "        model, BATCH_SIZE, test_sql_data, test_table_data, TEST_ENTRY))\n",
    "print(\"Test execution acc: %s\"%epoch_exec_acc(\n",
    "        model, BATCH_SIZE, test_sql_data, test_table_data, TEST_DB))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
